# Example configuration for running OpenEvolve with a locally hosted Ollama model
# Make sure the Ollama daemon is running and the desired model has been pulled.
# You still need to export OPENAI_API_KEY to any non-empty value (e.g. "ollama").

max_iterations: 50
checkpoint_interval: 10
log_level: "INFO"

llm:
  api_base: "http://localhost:11434/v1"
  models:
    - name: "qwen3-coder:30b"
      weight: 1.0
  temperature: 0.4
  top_p: 0.9
  max_tokens: 2048
  timeout: 120
  retries: 3
  retry_delay: 5

prompt:
  system_message: "You are an expert software engineer. Improve the candidate program while keeping it correct, readable, and efficient."

database:
  population_size: 100
  archive_size: 30
  num_islands: 3
  elite_selection_ratio: 0.2
  exploration_ratio: 0.3
  exploitation_ratio: 0.7

evaluator:
  timeout: 120
  max_retries: 2
  parallel_evaluations: 2
  cascade_evaluation: false

diff_based_evolution: true
max_code_length: 12000
